# Posture-Guided Image Synthesis of a Person

This project aims to implement motion transfer from a source video to a target person using PyTorch. The process follows the paper Everybody Dance Now by Chan et al. (ICCV 2019) and includes various stages to progressively understand Generative Adversarial Networks (GANs). The following sections describe the first two implementations: Nearest Neighbor Search and Direct Neural Network.

## 1. Nearest Neighbor Search

The nearest neighbor approach identifies the frame in the dataset with the skeleton closest to the desired skeleton.

### 1.1 Principle

- Input: A skeleton (ske) representing the target posture.
- Output: The image corresponding to the closest skeleton in the dataset.

This method searches through all skeletons in the target video dataset and computes the Euclidean distance between the input skeleton and each skeleton in the dataset. The image associated with the closest skeleton is selected.

### 1.2 Implementation

The implementation is handled in the `GenNearest` class:

- `__init__` Method: Initializes the generator with a VideoSkeleton object containing the dataset.
- generate Method:
  - Iterates through all skeletons in the dataset.
  - Finds the skeleton with the smallest distance to the input skeleton.
  - Returns the image corresponding to the closest skeleton.

### 1.4 Run

For the demo with trained model:

```bash
    python3 DanceDemo.py 1
```

To train a model:

```bash
    python3 GenVanillaNN.py
```

This will train the model for 2000 epoch (you need to change the number directly in the .py script if you want to).

## 2. Direct Neural Network

The direct neural network method trains a simple neural network to generate an image from an input skeleton coordinates.

### 2.1 Principle

- Input: A skeleton (ske) reduced to 26 features (13 joints in 2D).
- Output: An image generated by the neural network.

The network uses transposed convolutional layers (ConvTranspose2d) to progressively upsample a tensor from a shape of (batch_size, 26, 1, 1) to (batch_size, 3, 128, 128).

### 2.2 Implementation

The implementation is handled in the GenVanillaNN class:

- Training:
  - A VideoSkeletonDataset is prepared with pairs of skeletons coordinate and images.
  - The network is trained using a mean squared error (MSE) loss between generated images and target images.
  - Optimizer: Adam with a learning rate of 0.0001.
  - The network is saved to a file after training.
- Generation:
  - Takes a skeleton coordinate as input.
  - Produces an image of the target person in the corresponding posture.

### 2.3 Run

For the demo with trained model (200 epochs):

```bash
    python3 DanceDemo 2
```

To train the model:

```bash
    python3 GenVannillaNN.py
```

It will by default train for 2000 epochs.

## 3. Direct Neural Network with images as input

Same as 2. but take a skeleton image as input instead of coordinate.

### 3.1 Principle

- Input: A skeleton image (ske) reduced to 26 features (13 joints in 2D).
- Output: An image generated by the neural network.

The network uses convolutional layers (Conv2d) to progressively downsample a tensor from a shape of (batch_size, 3, 128, 128) and then reconstruct an output of shape (batch_size, 3, 128, 128).

### 3.2 Implementation

The implementation is handled in the GenVanillaNN class:

- Training:
  - A VideoSkeletonDataset is prepared with pairs of skeletons and images.
  - The network is trained using a L1 loss between generated images and target images.
  - Optimizer: Adam with a learning rate of 0.0001.
  - The network is saved to a file after training.
- Generation:
  - Takes a skeleton image as input.
  - Produces an image of the target person in the corresponding posture.

### 3.3 Run

For the demo with trained model (XX epoch):

```bash
    python3 DanceDemo 3
```

To train the model:

```bash
    python3 GenVannillaNNImage.py
```

## 4. GAN

### 4.1 Principle

The GAN approach trains 2 competing neural networks:

- **Generator (G)**: Generates images from skeletons, aiming to produce realistic outputs.
- **Discriminator (D)**: Distinguishes between real images (from the dataset) and fake images (generated by the generator).

The generator improves by learning to produce images that fool the discriminator, while the discriminator learns to differentiate real images from generated ones. This adversarial process refines the quality of generated images.

### 4.2 Implementation

The implementation is handled in the `GenGAN.py` file:

#### Components

1. **Generator (G)**:
   - Converts input skeletons to images using a sequence of transposed convolutional layers.
   - Starts with skeleton input and progressively upsamples to generate 128x128 RGB images.

2. **Discriminator (D)**:
   - Distinguishes between real images from the dataset and generated images.
   - Uses a series of convolutional layers with leaky ReLU activations and batch normalization.

3. **Loss Functions**:
   - **Discriminator Loss**: Binary Cross-Entropy Loss (BCE) between predicted labels and real/fake labels.
   - **Generator Loss**: Combines BCE for fooling the discriminator and L1 loss for image quality.

4. **Training**:
   - Alternates between training the discriminator and the generator.
   - For every batch:
     - Train D with real and fake images.
     - Train G to improve image realism and fool D.

### 4.3 Run

For the demo with trained model (30 epochs):

```bash
    python3 DanceDemo 4
```

To train a model:

```bash
python3 GenGAN.py [n_epochs] [--test|--notest] [batch_size] [loadSaved]
```

You have 3 optionnal arguments:

- nb_epoch the number of epoch you want.
- --test will test the model at the end of training while --notest will not..
- batch_size the batch size you want.
- loadSaved, to train with saved model.

## Project Requirements

Using conda create and activate the environment:

```bash
conda env create -f environment.yml
conda activate deepan
```

Or manually download the requirements:

```bash
pip install -r requirements.txt
```

It is possible that mediapipe does not install correctly with the conda installation, then use the pip install command above.

We mention that the version of python use for this project is Python 3.10.15.

## Author

MUNOZ Mat√©o, 12002495
CHAKRANE Ismail, 12413459
